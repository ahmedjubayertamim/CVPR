{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597db184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "Dataset exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "\n",
    "DATA_DIR = r\"E:\\Download\\CVPRStudentDataset\"   \n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPOCHS_HEAD = 10\n",
    "EPOCHS_FINE = 10\n",
    "\n",
    "MODEL_OUT = \"face_resnet_model.h5\"\n",
    "CLASSES_OUT = \"class_names.txt\"\n",
    "\n",
    "print(\"Dataset exists:\", os.path.isdir(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232fd4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed junk files: 0\n",
      "Checked: 659 | Removed: 0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "ALLOWED_EXT = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n",
    "\n",
    "def clean_corrupted_images(data_dir: str):\n",
    "    removed = 0\n",
    "    checked = 0\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for fn in files:\n",
    "            if not fn.lower().endswith(ALLOWED_EXT):\n",
    "                continue\n",
    "            path = os.path.join(root, fn)\n",
    "            checked += 1\n",
    "            try:\n",
    "                with Image.open(path) as im:\n",
    "                    im.verify()\n",
    "            except Exception:\n",
    "                print(\"Removing corrupted image:\", path)\n",
    "                try:\n",
    "                    os.remove(path)\n",
    "                    removed += 1\n",
    "                except Exception as e:\n",
    "                    print(\"   Could not remove:\", e)\n",
    "    print(f\"Checked: {checked} | Removed: {removed}\")\n",
    "\n",
    "def remove_junk_files(data_dir: str):\n",
    "    junk = {\"Thumbs.db\", \".DS_Store\"}\n",
    "    removed = 0\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for fn in files:\n",
    "            if fn in junk:\n",
    "                path = os.path.join(root, fn)\n",
    "                try:\n",
    "                    os.remove(path)\n",
    "                    removed += 1\n",
    "                    print(\"ğŸ—‘ï¸ Removed junk:\", path)\n",
    "                except Exception as e:\n",
    "                    print(\"   Could not remove:\", e)\n",
    "    print(\"Removed junk files:\", removed)\n",
    "\n",
    "\n",
    "remove_junk_files(DATA_DIR)\n",
    "clean_corrupted_images(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f805b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 532 images belonging to 39 classes.\n",
      "Found 126 images belonging to 39 classes.\n",
      "num_classes: 39\n",
      "Missing files: 0\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.08,\n",
    "    height_shift_range=0.08,\n",
    "    zoom_range=0.10,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_gen.num_classes\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# Safety: ensure generator filepaths exist (fixes FileNotFoundError during fit)\n",
    "missing = [p for p in (train_gen.filepaths + val_gen.filepaths) if not os.path.exists(p)]\n",
    "print(\"Missing files:\", len(missing))\n",
    "if missing:\n",
    "    print(\"Example missing path:\", missing[0])\n",
    "    raise FileNotFoundError(\"Some generator filepaths are missing. Rebuild dataset / re-run generators.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af954cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: class_names.txt\n",
      "First classes: ['21-45902-3', '22-40000-3', '22-46138-1', '22-46141-1', '22-46156-1', '22-46258-1', '22-46275-1', '22-46473-1', '22-46590-1', '22-46679-1']\n"
     ]
    }
   ],
   "source": [
    "idx_to_class = {v: k for k, v in train_gen.class_indices.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "with open(CLASSES_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in class_names:\n",
    "        f.write(c + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", CLASSES_OUT)\n",
    "print(\"First classes:\", class_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2317a3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,023</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     â”‚    \u001b[38;5;34m23,587,712\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m524,544\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m)             â”‚        \u001b[38;5;34m10,023\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,122,279</span> (92.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,122,279\u001b[0m (92.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">534,567</span> (2.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m534,567\u001b[0m (2.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = ResNet50(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    ")\n",
    "\n",
    "# Stage 1: Freeze backbone\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210486b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.1251 - loss: 3.7373       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 8s/step - accuracy: 0.2312 - loss: 3.0511 - val_accuracy: 0.7063 - val_loss: 1.5588\n",
      "Epoch 2/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5760 - loss: 1.6614  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 8s/step - accuracy: 0.6485 - loss: 1.4168 - val_accuracy: 0.8413 - val_loss: 0.7068\n",
      "Epoch 3/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.7834 - loss: 0.7982  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 12s/step - accuracy: 0.7650 - loss: 0.8403 - val_accuracy: 0.9365 - val_loss: 0.4059\n",
      "Epoch 4/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8501 - loss: 0.5693  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 6s/step - accuracy: 0.8647 - loss: 0.5488 - val_accuracy: 0.9603 - val_loss: 0.2869\n",
      "Epoch 5/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9219 - loss: 0.3651  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 4s/step - accuracy: 0.9154 - loss: 0.3549 - val_accuracy: 0.9921 - val_loss: 0.1657\n",
      "Epoch 6/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 4s/step - accuracy: 0.9417 - loss: 0.2691 - val_accuracy: 0.9444 - val_loss: 0.2402\n",
      "Epoch 7/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9437 - loss: 0.2513 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.9417 - loss: 0.2463 - val_accuracy: 0.9762 - val_loss: 0.1346\n",
      "Epoch 8/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9738 - loss: 0.1642 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.9662 - loss: 0.1628 - val_accuracy: 0.9841 - val_loss: 0.1226\n",
      "Epoch 9/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9620 - loss: 0.1629 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4s/step - accuracy: 0.9718 - loss: 0.1242 - val_accuracy: 0.9841 - val_loss: 0.0907\n",
      "Epoch 10/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.9850 - loss: 0.0993 - val_accuracy: 0.9841 - val_loss: 0.1102\n",
      "âœ… Stage 1 complete. Best model saved to: face_resnet_model.h5\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(MODEL_OUT, save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_HEAD,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"âœ… Stage 1 complete. Best model saved to:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5c6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 4s/step - accuracy: 0.9305 - loss: 0.3024 - val_accuracy: 0.9603 - val_loss: 0.1668\n",
      "Epoch 2/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 4s/step - accuracy: 0.9887 - loss: 0.0721 - val_accuracy: 0.9683 - val_loss: 0.1216\n",
      "Epoch 3/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 4s/step - accuracy: 0.9944 - loss: 0.0390 - val_accuracy: 0.9683 - val_loss: 0.1080\n",
      "âœ… Fine-tuning complete. Best model saved to: face_resnet_model.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_gen.reset()\n",
    "val_gen.reset()\n",
    "\n",
    "# Stage 2: Fine-tune\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze early layers; train last 50 layers (adjust if needed)\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "fine_history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_FINE,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\" Fine-tuning complete. Best model saved to:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af73bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 93.65%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"Validation accuracy: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb8b818-92dc-4631-9e9d-85fb328c03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ccd5e54-f405-455b-853d-6022ffba3896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exists: True\n",
      "Classes exists: True\n",
      "Attendance path: C:\\Users\\DELL\\attendance.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_PATH = \"face_resnet_model.h5\"\n",
    "CLASSES_TXT = \"class_names.txt\"\n",
    "ATT_CSV = \"attendance.csv\"      # saves in notebook folder\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "CONF_THRESH = 0.70              # adjust: 0.60 ~ 0.85 depending on your model\n",
    "\n",
    "print(\"Model exists:\", os.path.exists(MODEL_PATH))\n",
    "print(\"Classes exists:\", os.path.exists(CLASSES_TXT))\n",
    "print(\"Attendance path:\", os.path.abspath(ATT_CSV))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6877b18-2f1a-4a9f-b004-965923bd6eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model âœ…\n",
      "Num classes: 39\n",
      "First classes: ['21-45902-3', '22-40000-3', '22-46138-1', '22-46141-1', '22-46156-1']\n"
     ]
    }
   ],
   "source": [
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "with open(CLASSES_TXT, \"r\", encoding=\"utf-8\") as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(\"Loaded model \")\n",
    "print(\"Num classes:\", len(class_names))\n",
    "print(\"First classes:\", class_names[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f36edad-f051-4824-8f86-093e6a283049",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "def preprocess_face(face_bgr):\n",
    "    face = cv2.resize(face_bgr, IMG_SIZE)\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    face = preprocess_input(face)              # ResNet preprocess\n",
    "    return np.expand_dims(face, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc888f6-778a-4383-b683-f68eaeb983cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_attendance(student_id: str) -> bool:\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime(\"%Y-%m-%d\")\n",
    "    time_str = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(ATT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame(columns=[\"student_id\", \"date\", \"time\"])\n",
    "\n",
    "    already = ((df[\"student_id\"] == student_id) & (df[\"date\"] == date_str)).any()\n",
    "    if already:\n",
    "        return False\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame([{\n",
    "        \"student_id\": student_id,\n",
    "        \"date\": date_str,\n",
    "        \"time\": time_str\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    df.to_csv(ATT_CSV, index=False)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd4fc0e-3cff-4a93-9f57-95b4880181a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls: M=mark | Q=quit\n",
      "âš ï¸ Already marked today.\n",
      "Saved attendance to: C:\\Users\\DELL\\attendance.csv\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "last_label = \"No face\"\n",
    "last_conf = 0.0\n",
    "\n",
    "print(\"Controls: M=mark | Q=quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Camera read failed.\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.2, 5)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        (x, y, w, h) = max(faces, key=lambda b: b[2]*b[3])\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "\n",
    "        probs = model.predict(preprocess_face(face), verbose=0)[0]\n",
    "        idx = int(np.argmax(probs))\n",
    "        conf = float(probs[idx])\n",
    "\n",
    "        if conf >= CONF_THRESH and idx < len(class_names):\n",
    "            last_label = class_names[idx]\n",
    "            last_conf = conf\n",
    "        else:\n",
    "            last_label = \"Unknown\"\n",
    "            last_conf = conf\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    else:\n",
    "        last_label = \"No face\"\n",
    "        last_conf = 0.0\n",
    "\n",
    "    cv2.putText(frame, f\"{last_label} ({last_conf:.2f})\",\n",
    "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Attendance (ResNet50)\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k in [ord(\"q\"), ord(\"Q\")]:\n",
    "        break\n",
    "\n",
    "    if k in [ord(\"m\"), ord(\"M\")]:\n",
    "        if last_label not in [\"Unknown\", \"No face\"]:\n",
    "            ok = mark_attendance(last_label)\n",
    "            print(\" Marked!\" if ok else \" Already marked today.\")\n",
    "        else:\n",
    "            print(\" Unknown / No face -> not marked.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Saved attendance to:\", os.path.abspath(ATT_CSV))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fccd85f1-cb5e-4679-bfa8-c7baf15e1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(ATT_CSV):\n",
    "    df = pd.read_csv(ATT_CSV).sort_values([\"date\", \"time\"], ascending=False)\n",
    "    df\n",
    "else:\n",
    "    print(\"No attendance.csv found yet.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (digit_webcam)",
   "language": "python",
   "name": "digit_webcam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
